name: chatbot
services:
  db:
    image: timescale/timescaledb-ha:pg16
    container_name: chatbot-db
    environment:
      POSTGRES_PASSWORD: postgres
    ports:
      - "5434:5432"
    volumes:
      - ./data/db:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  storage:
    image: quay.io/minio/minio
    container_name: chatbot-minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: SPX9PsPwUzJzNcRHRn+RxKDJmnO515TukfKVBotfhng=
    volumes:
      - ./data/minio:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  searxng:
    image: searxng/searxng:latest
    container_name: chatbot-searxng
    ports:
      - "8081:8080"
    volumes:
      - ./searxng:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=http://localhost:8081/
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/healthz"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  vllm:
    image: vllm/vllm-openai:latest
    container_name: chatbot-vllm
    ports:
      - "11436:8000"  # vLLM main inference port
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
    command: >
      --model hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4
      --quantization awq
      --dtype auto
      --enable-auto-tool-choice
      --tool-call-parser llama3_json
      --host 0.0.0.0
      --port 8000
      --gpu-memory-utilization 0.8
      --max-model-len 8192
      --max-num-seqs 4
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    shm_size: '4gb'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Uncomment if you want embeddings on separate GPU/CPU
  embeddings:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.2
    container_name: chatbot-embeddings
    ports:
      - "11435:80"
    volumes:
      - ~/.cache/huggingface:/data
    environment:
      - MODEL_ID=BAAI/bge-small-en-v1.5
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
    command: --model-id BAAI/bge-small-en-v1.5 --port 80
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Neo4j for graph storage
  neo4j:
    image: neo4j:5.15.0
    container_name: chatbot-neo4j
    environment:
      NEO4J_AUTH: neo4j/${NEO4J_PASSWORD:-password}
      NEO4J_PLUGINS: '["apoc", "graph-data-science"]'
      NEO4J_dbms_security_procedures_unrestricted: apoc.*,gds.*
      NEO4J_dbms_memory_heap_max__size: 2G
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    volumes:
      - ./data/neo4j:/data
      - ./data/neo4j-logs:/logs
    healthcheck:
      test: ["CMD-SHELL", "cypher-shell -u neo4j -p ${NEO4J_PASSWORD:-password} 'RETURN 1'"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Next.js Application
  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: chatbot-app
    environment:
      - NODE_ENV=development
      - DATABASE_URL=postgresql://postgres:postgres@db:5432/postgres
      - POSTGRES_URL=postgresql://postgres:postgres@db:5432/postgres
      - AUTH_SECRET=${AUTH_SECRET}
      - NEXTAUTH_URL=http://localhost:3000
      - AUTH_TRUST_HOST=http://localhost:3000
      # vLLM Configuration
      - LLM_BASE_URL=http://vllm:8000/v1
      - LLM_PROVIDER=vllm
      - VLLM_EMBEDDING_URL=http://embeddings:80
      - VLLM_EMBEDDING_MODEL=BAAI/bge-small-en-v1.5
      - EMBEDDING_MODEL=BAAI/bge-small-en-v1.5
      # Services
      - SEARXNG_URL=http://searxng:8080
      - MINIO_URL=storage
      - MINIO_PORT=9000
      - MINIO_ACCESS_KEY=minio
      - MINIO_SECRET_KEY=SPX9PsPwUzJzNcRHRn+RxKDJmnO515TukfKVBotfhng=
      - MINIO_BUCKET=resources
      # Neo4j
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=${NEO4J_PASSWORD:-password}
      # Windows File Share (for MCP)
      - WINDOWS_SERVER=${WINDOWS_SERVER}
      - WINDOWS_SHARE=${WINDOWS_SHARE}
      - WINDOWS_USERNAME=${WINDOWS_USERNAME}
      - WINDOWS_PASSWORD=${WINDOWS_PASSWORD}
      - MOUNT_POINT=/mnt/windows-share
      # Email (Resend)
      - RESEND_API_KEY=${RESEND_API_KEY}
      - RESEND_FROM_EMAIL=${RESEND_FROM_EMAIL:-Security Monitor <security@localhost>}
      - SECURITY_ALERT_RECIPIENTS=${SECURITY_ALERT_RECIPIENTS:-admin@localhost}
    ports:
      - "3000:3000"
    volumes:
      - ./:/app
      - /app/node_modules
      - /app/.next
    depends_on:
      db:
        condition: service_healthy
      vllm:
        condition: service_healthy
      embeddings:
        condition: service_healthy
      searxng:
        condition: service_healthy
      storage:
        condition: service_healthy
      neo4j:
        condition: service_healthy
    restart: unless-stopped
    command: npm run dev

  # Cron service for scheduled workflows
  cron:
    image: alpine:3.18
    container_name: chatbot-cron
    environment:
      - APP_URL=http://app:3000
    volumes:
      - ./scripts/cron:/scripts:ro
    depends_on:
      - app
    entrypoint: /scripts/cron-entrypoint.sh
    restart: unless-stopped

  # Caddy reverse proxy (optional)
  caddy:
    image: caddy:2.7-alpine
    container_name: chatbot-caddy
    ports:
      - "80:80"
      - "443:443"
      - "443:443/udp"  # HTTP/3
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - ./data/caddy:/data
      - ./data/caddy-config:/config
    depends_on:
      - app
    restart: unless-stopped
