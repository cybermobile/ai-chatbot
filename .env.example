# Generate a random secret: `openssl rand -base64 32`
AUTH_SECRET=****

# Instructions to create a database here: https://github.com/timescale/pgai/blob/main/docs/vectorizer-quick-start.md
# For local development with docker-compose: postgresql://postgres:postgres@localhost:5434/postgres
# NOTE: Database must have pgvector extension enabled. TimescaleDB includes this by default.
POSTGRES_URL=****

# Instructions to create a MinIO instance here: https://min.io/docs/minio/container/administration/console/security-and-access.html#minio-console-user-access-keys
# For local development with docker-compose, use these values:
MINIO_URL=localhost
MINIO_PORT=9000
MINIO_ACCESS_KEY=minio
MINIO_SECRET_KEY=password
MINIO_BUCKET=resources

# next-auth trust host
NEXTAUTH_URL=http://localhost:3000
AUTH_TRUST_HOST=http://localhost:3000

# LLM Inference Backend (vLLM or Ollama)
# For vLLM (recommended for Linux with GPU): http://127.0.0.1:11434
# For Ollama (Mac): http://127.0.0.1:11434
LLM_BASE_URL=http://127.0.0.1:11434
LLM_PROVIDER=vllm  # Options: vllm, ollama

# vLLM Embedding Server (Linux with GPU)
VLLM_EMBEDDING_URL=http://127.0.0.1:11435
VLLM_EMBEDDING_MODEL=BAAI/bge-small-en-v1.5

# Ollama configuration (Mac only, fallback)
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# Optional: HuggingFace token for gated models (Llama, etc.)
HF_TOKEN=

# SearXNG web search (optional, defaults to localhost:8080)
SEARXNG_URL=http://localhost:8080

# ========================================
# Vercel Workflow & MCP Configuration
# ========================================

# Windows File Share Access (for MCP Filesystem Server)
# Required for security monitoring and RAG ingestion workflows
# Used in LOCAL DEVELOPMENT with stdio transport
WINDOWS_SERVER=192.168.1.100
WINDOWS_SHARE=Logs
WINDOWS_USERNAME=admin
WINDOWS_PASSWORD=your_secure_password
MOUNT_POINT=/mnt/windows-share

# MCP Server (for PRODUCTION deployment)
# Required for Vercel production - see MCP_PRODUCTION_DEPLOYMENT.md
# Leave empty for local development (uses stdio transport automatically)
MCP_SERVER_URL=https://your-mcp-server.vercel.app/api/mcp/filesystem
MCP_SERVER_API_KEY=your_secret_api_key

# Email Configuration (Resend)
# Sign up at https://resend.com and get your API key
# Integrate with Vercel: https://vercel.com/integrations/resend
RESEND_API_KEY=re_xxxxxxxxxxxxxxxxxxxxxxxxxxxx
RESEND_FROM_EMAIL=Security Monitor <security@yourdomain.com>
SECURITY_ALERT_RECIPIENTS=security@company.com,admin@company.com

# Workflow Configuration
# Embedding model for RAG ingestion (nomic-embed-text for Ollama, BAAI/bge-small-en-v1.5 for vLLM)
EMBEDDING_MODEL=nomic-embed-text